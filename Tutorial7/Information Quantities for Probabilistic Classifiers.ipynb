{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d683068",
   "metadata": {},
   "source": [
    "# Information Quantities for Probabilistic Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b5dee",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**CS5483 Data Warehousing and Data Mining**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1267d9db",
   "metadata": {},
   "source": [
    "This notebook will introduce the information quantities often used for training probabilistic classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bf73ce",
   "metadata": {},
   "source": [
    "As an example, the following handwritten digit classifier is [trained by deep learning](https://www.cs.cityu.edu.hk/~ccha23/deepbook/divedeep.html) using cross entropy loss:\n",
    "1. Handwrite a digit from 0, ..., 9.\n",
    "1. Click predict to see if the app can recognize the digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4324095",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://www.cs.cityu.edu.hk/~ccha23/mnist/\" width=\"805\" height=\"450\" style=\"border:none;\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09980063",
   "metadata": {},
   "source": [
    "## Information Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309aeb97",
   "metadata": {},
   "source": [
    "A fundamental property of mutual information is that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4e9a9",
   "metadata": {},
   "source": [
    "````{admonition} Theorem (Positivity of mutual information)\n",
    "\n",
    ":label: MI-positivity\n",
    "\n",
    "$I(X;Y)\\geq 0$ with equality iff $X$ and $Y$ are independent.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424e3f6",
   "metadata": {},
   "source": [
    "To show this, we think of the mutual information as a statistical distance called the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a3998f",
   "metadata": {},
   "source": [
    "````{admonition} Definition \n",
    "\n",
    "The (information/KL) divergence between two probability measures $P$ and $Q$ over $\\mathcal{Z}$ is defined as\n",
    "\n",
    "$$\n",
    "D(P\\|Q) := \\int_{\\mathcal{Z}} (dP) \\log \\frac{dP}{dQ}.\n",
    "$$ (D)\n",
    "\n",
    "The conditional divergence is defined as \n",
    "\n",
    "$$\n",
    "D(V\\|W|U):=D(U\\times V\\| U\\times W).\n",
    "$$\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54909de3",
   "metadata": {},
   "source": [
    "For the information divergence to be called a [divergence](https://en.wikipedia.org/wiki/Divergence_(statistics)), it has to satisfy the following property:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e16613",
   "metadata": {},
   "source": [
    "````{admonition} Lemma (Positivity of divergence)\n",
    "\n",
    ":label: D-positivity\n",
    "\n",
    "$D(P\\|Q) \\geq 0$, with equality iff $P=Q$ almost everywhere.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19feee04",
   "metadata": {},
   "source": [
    "````{admonition} Proof \n",
    "\n",
    "Without loss of generality, we can rewrite the divergence as the expectation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D(P_{Z}\\|P_{Z'}) &:= E\\left[ \\frac{p_{Z}(Z')}{p_{Z'}(Z')} \\log\\frac{p_{Z}(Z')}{p_{Z'}(Z')} \\right]\\\\\n",
    "&\\geq E\\left[ \\frac{p_{Z}(Z')}{p_{Z'}(Z')}\\right] \\log \\underbrace{E\\left[\\frac{p_{Z}(Z')}{p_{Z'}(Z')} \\right]}_{=1} = 0,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the last inequality follows from [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality) and the convexity of $r \\mapsto r \\log r$. Since $r$ is strictly convex, the inequality holds iff $\\frac{p_{Z}(Z')}{p_{Z'}(Z')}=1$ almost surely, i.e., $P_{Z}=P_{Z'}$ almost everywhere.  \n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc8fc2",
   "metadata": {},
   "source": [
    "**Exercise** Prove the positivity of mutual information ({prf:ref}`MI-positivity`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3974bf7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56b7e14126c993c2956d0899146fd5df",
     "grade": true,
     "grade_id": "MI-positivity",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a47e06",
   "metadata": {},
   "source": [
    "## Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355343c8",
   "metadata": {},
   "source": [
    "A probabilistic classifier returns a conditional probability estimate $\\hat{P}_{Y|X}$ as a function of the training data $S$, which consists of i.i.d. samples of $(X,Y)$ but independent of $(X,Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5c341",
   "metadata": {},
   "source": [
    "A sensible choice of the loss function is\n",
    "\n",
    "$$\\ell(\\hat{P}_{Y|X}(\\cdot|x), P_{Y|X}(\\cdot|x)):=D(\\hat{P}_{Y|X}(\\cdot|x)\\|P_{Y|X}(\\cdot|x))$$ (D-loss)\n",
    "\n",
    "because, by the positivity of divergence (Lemma {prf:ref}`D-positivity`), the above loss is non-negative and equal to $0$ iff $P_X\\times \\hat{P}_{Y|X}=P_X \\times P_{Y|X}$ almost surely. Using this loss function, we have a simple bias-variance trade-off:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de83c6e5",
   "metadata": {},
   "source": [
    "````{admonition} Theorem (Bias-variance trade-off)\n",
    "\n",
    "The expected loss (risk) for the loss function {eq}`D-loss` is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[D(\\hat{P}_{Y|X} \\| P_{Y|X}|P_X)] = \\overbrace{I(S;\\hat{Y}|X)}^{\\text{Variance}} + \\overbrace{D(E[\\hat{P}_{Y|X}]\\|P_{Y|X}|P_X)}^{\\text{Bias}}\n",
    "\\end{align}\n",
    "$$ (Bias-Variance)\n",
    "\n",
    "where $\\hat{Y}$ is distributed according to\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P_{X,Y,S,\\hat{Y}}&=P_{X,Y}\\times P_{S} \\times P_{\\hat{Y}|X,S} && \\text{where}\\\\\n",
    "P_{\\hat{Y}|X,S}(y|x,s) &= \\hat{P}_{Y|X}(y|x) && \\text{for }(x,y,s)\\in \\mathcal{X}\\times \\mathcal{Y}\\times \\mathcal{S}.\n",
    "\\end{align}\n",
    "$$ (Yhat)\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b884b9",
   "metadata": {},
   "source": [
    "- The variance $I(S;\\hat{Y}|X)$ (also $I(S;X,\\hat{Y})$ as $I(S;X)=0$) reflects the level of overfitting as it measures how much the estimate depends on the training data.\n",
    "- The bias $D(E[\\hat{P}_{Y|X}]\\|P_{Y|X}|P_X)$ reflects the level of underfitting as it measures how much the expected estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec182a0f",
   "metadata": {},
   "source": [
    "````{admonition} Proof \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[D(\\hat{P}_{Y|X} \\| P_{Y|X}|P_X)] \n",
    "&= E\\left[\\log \\frac{\\hat{p}_{Y|X}(\\hat{Y}|X)}{p_{Y|X}(\\hat{Y}|X)}\\right]\\\\\n",
    "&= \\underbrace{E\\left[\\log \\frac{\\hat{p}_{Y|X}(\\hat{Y}|X)}{E[\\hat{p}_{Y|X}](\\hat{Y}|X)}\\right]}_{\\text{(i)}} + \\underbrace{E\\left[\\log \\frac{E[\\hat{p}_{Y|X}](\\hat{Y}|X)]}{p_{Y|X}(\\hat{Y}|X)}\\right]}_{=D(E[\\hat{P}_{Y|X}]\\|P_{Y|X}|P_X) \\text{(bias)}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "It remains to show (i) is the variance. By {eq}`Yhat`,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[\\hat{p}_{Y|X}](y|x)\n",
    "&= E[p_{\\hat{Y}|X,S}(y|x,S)|X=x]\\\\\n",
    "&= p_{\\hat{Y}|X}(y|x).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Substituting {eq}`Yhat` and the above into (i), we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{(i)} &= E\\left[\\log \\frac{p_{\\hat{Y}|X,S}(\\hat{Y}|X,S)}{p_{\\hat{Y}|X}(\\hat{Y}|X)}\\right]\\\\\n",
    "&= I(S;\\hat{Y}|X),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which completes the proof.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e38d5c",
   "metadata": {},
   "source": [
    "The loss in {eq}`D-loss`, however, cannot be evaluated on $S$ for training because $P_{Y|X}(\\cdot|x_i)$ is not known. Instead, we often use the [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) loss\n",
    "\n",
    "$$\n",
    "\\ell(\\hat{P}_{Y|X}(\\cdot |x),y) := \\log \\frac{1}{\\hat{p}_{Y|X}(y|x)}.\n",
    "$$ (CE-loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8263b78",
   "metadata": {},
   "source": [
    "````{admonition} Theorem (Cross entropy)\n",
    "\n",
    "The risk for the loss in {eq}`CE-loss` is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E\\left[\\log \\frac1{\\hat{p}_{Y|X}(Y|X)}\\right] \n",
    "&= H(Y|X) + E[D(P_{Y|X}\\| \\hat{P}_{Y|X}|P_X)] \\\\\n",
    "&\\geq H(Y|X) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with equality iff $P_{X}\\times P_{Y|X}=P_{X}\\times \\hat{P}_{Y|X}$ almost everywhere.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ead25c",
   "metadata": {},
   "source": [
    "**Exercise** Prove the above result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2793c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e23993eaaf0123173a83f5ba8585c90a",
     "grade": true,
     "grade_id": "CE-loss",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
