{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8521483",
   "metadata": {},
   "source": [
    "# Information Quantities for Decision Tree Induction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5655e1",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "**CS5483 Data Warehousing and Data Mining**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90934b9f",
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install dit\n",
    "import dit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dit.shannon import conditional_entropy, entropy, mutual_information\n",
    "from IPython.display import Math, display\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57399915",
   "metadata": {},
   "source": [
    "In this notebook, we will use the [dit package](https://dit.readthedocs.io/en/latest/) to compute fundamental information quantities used in the decision tree induction. More basic implementations and interpretations are given [here](https://www.cs.cityu.edu.hk/~ccha23/cs1302book/Lab8/Information%20Theory.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea78395",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cbd9b2",
   "metadata": {},
   "source": [
    "Consider any $(X, Y)$ taking values from $\\mathcal{X}\\times \\mathcal{Y}$ according to some joint probability measure $P_{X,Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452bbba",
   "metadata": {},
   "source": [
    "````{admonition} Definition \n",
    "\n",
    "Define\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(Y) &:= E\\left[\\log \\tfrac1{p_{Y}(Y)}\\right] && \\text{called the entropy of $Y$}\\\\\n",
    "H(Y|X) &:= E\\left[\\log \\tfrac1{p_{Y|X}(Y|X)}\\right] && \\text{called conditional the entropy of $Y$ given $X$,}\n",
    "\\end{align}\n",
    "$$ (entropy)\n",
    "\n",
    "where $p_{Y|X}$ and $p_{Y}$ are the probability density functions of $Y$ with and without conditioning on $X$ respectively. The joint entropy $H(X,Y)$ of $X$ and $Y$ is defined as the entropy of the random tuple $(X,Y)$ using the joint density $p_{X,Y}$.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af86f755",
   "metadata": {},
   "source": [
    "````{important}\n",
    "\n",
    "- Unless otherwise specified, all the logarithm is base 2, in which case the information quantities are in the unit of bit (binary digit).  \n",
    "- A popular alternative is to use the natural logarithm, $\\log = \\ln$, in which case the unit is in *nat*.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f7085b",
   "metadata": {},
   "source": [
    "To make sense of the expectations in {eq}`entropy`, it is important to see that $p_Y(Y)$ and $p_{Y|X}(Y|X)$ are random because \n",
    "- their arguments, namely $X$ and $Y$, are random \n",
    "- even though $p_Y$ and $p_{Y|X}$ are deterministic functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af70b2c",
   "metadata": {},
   "source": [
    "For discrete random variable $Y$, $p_Y$ and $p_{Y|X}$ in {eq}`entropy` should be the probability mass functions:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(Y) &= \\sum_{y\\in \\mathcal{Y}:p_Y(y)>0} p_Y(y) \\log \\frac1{p_Y(y)}\\\\\n",
    "H(Y|X) &= E\\left[\\sum_{y\\in \\mathcal{Y}:p_{Y|X}(y)>0} p_{Y|X}(y|X) \\log \\frac1{p_{Y|X}(y|X)}\\right]\n",
    "\\end{align}\n",
    "$$ (entropy-drv)\n",
    "\n",
    "where $\\mathcal{Y}$ is the alphabet set of $Y$. The last expectation is over the randomness of $X$, which needs not be discrete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e933d",
   "metadata": {},
   "source": [
    "It is convenient to express the entropy as a function of the probability masses as\n",
    "\n",
    "$$\n",
    "h(p) = h(p_1, p_2, \\dots) := \\sum_{k:p_k>0} p_k \\log \\frac1{p_k}\n",
    "$$ (h)\n",
    "\n",
    "where $p_k\\in [0,1]$ for all $k$, $\\sum_k p_k=1$, and $p:i\\mapsto p_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731afc6",
   "metadata": {},
   "source": [
    "Rewriting the entropies in {eq}`entropy-drv` in terms of $h$, we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(Y) &= h(p_Y)\\\\\n",
    "H(Y|X) &= E[h(p_{Y|X}(\\cdot|X))].\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d848704",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "For discrete random variables $Y$, explain whether $H(Y)$ is non-negative and scale-invariant in the sense that \n",
    "\n",
    "$$\n",
    "0\\leq H(Y) = H(c Y) \\quad \\forall c\\neq 0,\n",
    "$$\n",
    "\n",
    "using the above formulae in {eq}`h`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d0669d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "487102cf44abdfd4edcf0d30d499a704",
     "grade": true,
     "grade_id": "scale-invariance",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0427fbf3",
   "metadata": {},
   "source": [
    "````{note}\n",
    "\n",
    "If $Y$ is a continuous random variable, $H(Y)$ is called the [differential entropy](https://en.wikipedia.org/wiki/Differential_entropy), which can be negative and may not be scale-invariant.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5384cc",
   "metadata": {},
   "source": [
    "````{admonition} Example \n",
    "\n",
    "Consider the following distribution.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_k=\\begin{cases}\n",
    "\\frac12 & k\\in \\{00\\}\\\\\n",
    "\\frac14 & k\\in \\{10, 11\\}\\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The entropy is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h(p_{00}, p_{01}, p_{10}, p_{11}) &= h\\left(\\frac12, 0, \\frac14, \\frac14 \\right)\\\\\n",
    "&=\\frac12 \\log \\frac21 +  \\frac14 \\log \\frac{4}{1} +  \\frac14 \\log \\frac{4}{1} \\\\\n",
    "&= 1.5\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6a6c84",
   "metadata": {},
   "source": [
    "To define and plot the distribution above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b4c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = dit.Distribution([\"00\", \"01\", \"10\", \"11\"], [1 / 2, 0, 1 / 4, 1 / 4])\n",
    "\n",
    "plt.stem(p.outcomes, p.pmf)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(r\"$p_k$\")\n",
    "plt.ylim((0, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81c07a",
   "metadata": {},
   "source": [
    "To compute the entropy of the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Math(\"h(p_{00}, p_{01}, p_{10}, p_{11})=\" + f\"{entropy(p):.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8facf0f6",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9ddee",
   "metadata": {},
   "source": [
    "````{admonition} Definition \n",
    "\n",
    "The [*mutual information*](https://en.wikipedia.org/wiki/Mutual_information) between $X$ and $Y$ is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(X;Y)\n",
    "&:= E\\left[\\log \\tfrac{p_{X,Y}(X,Y)}{p_X(X)p_Y(Y)}\\right]\\\\\n",
    "&= E\\left[\\log \\tfrac{p_{Y|X}(Y|X)}{p_Y(Y)}\\right]\\\\\n",
    "&= E\\left[\\log \\tfrac{p_{X|Y}(X|Y)}{p_X(X)}\\right],\n",
    "\\end{align}\n",
    "$$ (MI) \n",
    "\n",
    "where $p_{X,Y}$ and $p_{X}p_{Y}$ are the join density and the product of marginal density functions of $X$ and $Y$. The conditional mutual information of $X$ and $Y$ given $Z$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(X;Y)\n",
    "&:= E\\left[\\log \\tfrac{p_{X,Y|Z}(X,Y|Z)}{p_{X|Z}(X|Z)p_{Y|Z}(Y|Z)}\\right],\n",
    "\\end{align}\n",
    "$$ (CMI)\n",
    "\n",
    "which has the additional conditioning on $Z$.\n",
    "\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4c92ed",
   "metadata": {},
   "source": [
    "All the information quantities defined so far can be concisely related using a *Venn Diagram*:\n",
    "\n",
    "![Venn diagram](images/venn.dio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ab757",
   "metadata": {},
   "source": [
    "````{admonition} Theorem (Chain rules)\n",
    "\n",
    ":label: chain-rules\n",
    "\n",
    "The mutual information {eq}`MI` can be expressed in terms of the entropies as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(X;Y)&=H(Y)-H(Y|X)\\\\\n",
    "&=H(X)+H(Y)-H(X,Y)\\\\\n",
    "&=H(X)-H(X|Y).\n",
    "\\end{align}\n",
    "$$ (MI-H)\n",
    "\n",
    "The entropy {eq}`entropy` satisfies\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(X,Y)&=H(X)+H(Y|X)\\\\\n",
    "&=H(Y)+H(X|Y),\n",
    "\\end{align}\n",
    "$$ (H-chain-rules)\n",
    "\n",
    "which is called the chain rule of entropy.\n",
    "\n",
    "The conditional mutual information {eq}`CMI` satisfies\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(X;Y|Z)&=I(X,Z;Y) - I(X;Z) \\\\\n",
    "&= I(X; Y, Z) - I(Y;Z),\n",
    "\\end{align}\n",
    "$$ (I-chain-rules)\n",
    "\n",
    "which is called the chain rule of mutual information.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c10a2",
   "metadata": {},
   "source": [
    "````{admonition} Proof \n",
    "\n",
    "The proof is a straightforward application of the following facts:\n",
    "- $\\log ab = \\log a + \\log b$,\n",
    "- linearity of expectation, and\n",
    "- $p_{X,Y}=p_X p_{Y|X} = p_Y p_{X|Y}$.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187fbeaf",
   "metadata": {},
   "source": [
    "````{admonition} Example \n",
    "\n",
    "Rewrite the same distribution but using the random variables $X$ and $Y$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_{XY}(x,y)=\\begin{cases}\n",
    "\\frac12 & (x,y)\\in \\{(0,0)\\}\\\\\n",
    "\\frac14 & (x,y)\\in \\{(1,0), (1,1)\\}\\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To calculate the mutual information using the conditional entropy:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(Y) &= h(p_Y(0), p_Y(1))\\\\\n",
    "&= h(p_{00} + p_{10}, p_{01} + p_{11}) \\\\\n",
    "&= h(\\frac34, \\frac14) \\\\\n",
    "&\\approx \\underline{0.811}\\\\\n",
    "H(Y|X) &= p_X(0) h(p_{Y|X}(0|0),p_{Y|X}(1|0)) + p_X(1) h(p_{Y|X}(0|1),p_{Y|X}(1|1))\\\\\n",
    "&= (p_{00} + p_{01}) h\\left(\\frac{p_{00}}{p_{00} + p_{01}}, \\frac{p_{01}}{p_{00} + p_{01}} \\right) + (p_{10} + p_{11}) h\\left(\\frac{p_{10}}{p_{10} + p_{11}}, \\frac{p_{11}}{p_{10} + p_{11}} \\right)\\\\\n",
    "&= \\frac12 h\\left(1, 0 \\right) + \\frac12 h\\left(\\frac12,\\frac12 \\right) \\\\\n",
    "&= \\underline{0.5} = 1.5-1 = H(X,Y) - H(X)\\\\\n",
    "I(X;Y) &= H(Y)- H(Y|X) \\\\\n",
    "&\\approx 0.811 - 0.5\\\\\n",
    "&= \\underline{0.311} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To verify the chain rule $H(X,Y)=H(X)+H(Y|X)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(X, Y) -  H(Y|X) &= h(p_{00}, p_{01}, p_{10}, p_{11}) - 0.5 \\\\\n",
    "&= \\underline{1} = h\\left(\\frac12,\\frac12\\right) = H(X).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a87c4",
   "metadata": {},
   "source": [
    "To define random variables in dit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541636a3",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "rv_names = \"XY\"\n",
    "p.set_rv_names(rv_names)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59550ba9",
   "metadata": {},
   "source": [
    "We can now compute the entropies for different subsets of random variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rvs in [\"XY\", \"X\", \"Y\", \"\"]:\n",
    "    display(Math(f\"H({','.join([rv for rv in rvs])})=\" + f\"{entropy(p, rvs):.3g}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f1aaf3",
   "metadata": {},
   "source": [
    "To compute the conditional entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b76188",
   "metadata": {},
   "outputs": [],
   "source": [
    "Math(\"H(Y|X)=\" + f\"{conditional_entropy(p, 'Y', 'X'):.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43fb4e6",
   "metadata": {},
   "source": [
    "To compute the mutual information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f752c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Math(\"I(X;Y)=\" + f\"{mutual_information(p, 'X','Y'):.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f1b07a",
   "metadata": {},
   "source": [
    "**Exercise** Assign to `conditional_entropy_X_given_Y` the value of the conditional entropy $H(X|Y)$ for the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e50505",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42705bf06a0a84379111c2ff41562308",
     "grade": false,
     "grade_id": "conditional_entropy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# your python code here\n",
    "# end of python code\n",
    "\n",
    "conditional_entropy_X_given_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36aca6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68080871fc1800af44b43138296ca4be",
     "grade": true,
     "grade_id": "test-conditional_entropy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-output",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "assert (\n",
    "    0\n",
    "    <= conditional_entropy_X_given_Y\n",
    "    <= min(entropy(p, \"X\"), entropy(p, \"XY\") - mutual_information(p, \"X\", \"Y\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d0abc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c6c2983f40eeb5fe86e598e9c72d638",
     "grade": true,
     "grade_id": "htest-conditional_entropy",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e2263",
   "metadata": {},
   "source": [
    "## Decision Tree Induction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b430e7",
   "metadata": {},
   "source": [
    "Consider a dataset consisting of i.i.d. samples of some discrete random variables with an unknown joint distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c985d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\", dtype=str, skipinitialspace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a583e2",
   "metadata": {},
   "source": [
    "To estimate the information quantities of the features, we use the empirical distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084d30f",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "emp_p = dit.uniform([tuple(lst) for lst in df.to_numpy()])\n",
    "emp_p.set_rv_names(df.columns)\n",
    "emp_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5228246c",
   "metadata": {},
   "source": [
    "**How to determine which attribute is more informative?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666af2a5",
   "metadata": {},
   "source": [
    "$\\text{Info}(D)$ denotes the entropy $H(Y)$ of the empirical distribution of target $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "InfoD = entropy(emp_p, \"Y\")\n",
    "Math(r\"\\text{Info}(D)=\" + f\"{InfoD:.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4487ce99",
   "metadata": {},
   "source": [
    "$\\text{Info}_{X_i}(D)$ for different input feature $X_i$ denotes the conditional entropy $H(Y|X_i)$ of $Y$ given $X_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc1fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "InfoXD = {}\n",
    "for cond in [\"X1\", \"X2\", \"X3\", \"X4\"]:\n",
    "    InfoXD[cond] = conditional_entropy(emp_p, [\"Y\"], [cond])\n",
    "\n",
    "Math(\n",
    "    r\"\"\"\n",
    "\\begin{{aligned}}\n",
    "\\text{{Info}}_{{X_1}}(D)&={X1:.3g}\\\\\n",
    "\\text{{Info}}_{{X_2}}(D)&={X2:.3g}\\\\\n",
    "\\text{{Info}}_{{X_3}}(D)&={X3:.3g}\\\\\n",
    "\\text{{Info}}_{{X_4}}(D)&={X4:.3g}\\\\\n",
    "\\end{{aligned}}\n",
    "\"\"\".format(\n",
    "        **InfoXD\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1afafd1",
   "metadata": {},
   "source": [
    "**Exercise** The information gain $\\text{Gain}_{X_i}(D)$ can be calculated as the mutual information $I(X_i;Y):=H(Y)-H(Y|X_i)$. Assign to `GainXD` a dictionary similar to `InfoXD` but stores the information gains instead of conditional entropies for different input features. You may use the function `mutual_information` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a94c9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e78b662d2e13adb4491d2170c6879cd0",
     "grade": false,
     "grade_id": "Gain",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "GainXD = {}\n",
    "# your python code here\n",
    "# end of python code\n",
    "\n",
    "\n",
    "Math(\n",
    "    r\"\"\"\n",
    "\\begin{{aligned}}\n",
    "\\text{{Gain}}_{{X_1}}(D)&={X1:.3g}\\\\\n",
    "\\text{{Gain}}_{{X_2}}(D)&={X2:.3g}\\\\\n",
    "\\text{{Gain}}_{{X_3}}(D)&={X3:.3g}\\\\\n",
    "\\text{{Gain}}_{{X_4}}(D)&={X4:.3g}\\\\\n",
    "\\end{{aligned}}\n",
    "\"\"\".format(\n",
    "        **GainXD\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae71361",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58f0bf1f52271760f11f6372b0d1e00e",
     "grade": true,
     "grade_id": "test-Gain",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "assert np.isclose(GainXD[\"X1\"], 0.5, rtol=1e-3)\n",
    "assert np.isclose(GainXD[\"X2\"], 1, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b6b404",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2eaefebd14271ab92b676ad14350c170",
     "grade": true,
     "grade_id": "htest-Gain",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590eb5d9",
   "metadata": {},
   "source": [
    "**Exercise** Which attribute gives the highest information gain? Should we choose it as the splitting attribute?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f05152b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e636a548f742243280ce43d03d91000",
     "grade": true,
     "grade_id": "infogain",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35fd0e6",
   "metadata": {},
   "source": [
    "To avoid the bias towards attributes with many outcomes, C4.5/J48 normalizes information gain by $\\text{SplitInfo}_{X_i}(D)$, which can be calculated as $H(X_i)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "SplitInfoXD = {}\n",
    "for cond in [\"X1\", \"X2\", \"X3\", \"X4\"]:\n",
    "    SplitInfoXD[cond] = entropy(emp_p, [cond])\n",
    "\n",
    "Math(\n",
    "    r\"\"\"\n",
    "\\begin{{aligned}}\n",
    "\\text{{SplitInfo}}_{{X_1}}(D)&={X1:.3g}\\\\\n",
    "\\text{{SplitInfo}}_{{X_2}}(D)&={X2:.3g}\\\\\n",
    "\\text{{SplitInfo}}_{{X_3}}(D)&={X3:.3g}\\\\\n",
    "\\text{{SplitInfo}}_{{X_4}}(D)&={X4:.3g}\\\\\n",
    "\\end{{aligned}}\n",
    "\"\"\".format(\n",
    "        **SplitInfoXD\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56789821",
   "metadata": {},
   "source": [
    "**Exercise** Assign to GainRatioXD the dictionary of information gain ratios $\\frac{\\text{Gain}_{X_i}(D)}{\\text{SplitInfo}_{X_i}(D)}$ for different input features $X_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f6a10f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8180d1e72b39352ca86cc0bca02d78e",
     "grade": false,
     "grade_id": "GainRatio",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "GainRatioXD = {}\n",
    "# your python code here\n",
    "# end of python code\n",
    "\n",
    "\n",
    "Math(\n",
    "    r\"\"\"\n",
    "\\begin{{aligned}}\n",
    "\\frac{{\\text{{Gain}}_{{X_1}}(D)}}{{\\text{{SplitInfo}}_{{X_1}}(D)}}&={X1:.3g}\\\\\n",
    "\\frac{{\\text{{Gain}}_{{X_2}}(D)}}{{\\text{{SplitInfo}}_{{X_2}}(D)}}&={X2:.3g}\\\\\n",
    "\\frac{{\\text{{Gain}}_{{X_3}}(D)}}{{\\text{{SplitInfo}}_{{X_3}}(D)}}&={X3:.3g}\\\\\n",
    "\\frac{{\\text{{Gain}}_{{X_4}}(D)}}{{\\text{{SplitInfo}}_{{X_4}}(D)}}&={X4:.3g}\\\\\n",
    "\\end{{aligned}}\n",
    "\"\"\".format(\n",
    "        **GainRatioXD\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b1e62",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77dac7eeb1c74e680e7e02fded446b6c",
     "grade": true,
     "grade_id": "test-GainRatio",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "assert np.isclose(GainRatioXD[\"X1\"], 0.5, rtol=1e-3)\n",
    "assert np.isclose(GainRatioXD[\"X2\"], 1, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75db35ab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5cceeb0589b4498f9ec99af884de5be",
     "grade": true,
     "grade_id": "htest-GainRatio",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e254e284",
   "metadata": {},
   "source": [
    "**Exercise** Explain whether $X_4$ is a good splitting attribute compared to other input attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de6692",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e9d188661e23c703a0fd1cff8f3bd33",
     "grade": true,
     "grade_id": "ratio",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
